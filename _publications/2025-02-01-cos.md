---
title: "Fast Large Language Model Collaborative Decoding via Speculation"
collection: publications
category: conferences
permalink: /publication/2025-02-01-cos
excerpt: 'Collaborative decoding via Speculation (CoS) is a novel framework that accelerates the ensemble of any number of LLMs without sacrificing performance. It could reach 1.11x-2.23x over standard ensemble techniques on two-model or three-model pairs.'
date: 2025-02-01
venue: 'ICML'
authors: 'Jiale Fu*, Yuchu Jiang*, Junkai Chen, Jiaming Fan, Xin Geng, Xu Yang'
paperurl: 'https://arxiv.org/abs/2502.01662v1'
citation: 'Fu J, Jiang Y, Chen J, et al. Fast Large Language Model Collaborative Decoding via Speculation[J]. arXiv preprint arXiv:2502.01662, 2025.'
---

# 1. Introduction: The Need for Efficient Collaborative Inference

Collaborative decoding for Large Language Models (LLMs) enhances output quality by combining outputs from multiple models at each generation step. However, this approach incurs significant computational overhead, as standard collaborative methods require each model to perform a forward pass for every token, leading to a total cost of \\(O(nT)\\) for \\(n\\) models and \\(T\\) tokens.

We propose **Collaborative Decoding via Speculation (CoS)**, a novel framework that accelerates collaborative decoding by leveraging speculative decoding principles. CoS achieves **1.11×–2.23×** speedups across diverse settings without compromising generation quality. Its core innovations are: (1) sampling from a **combined distribution** of models rather than a single model, and (2) an **alternate proposal framework** that utilizes bonus tokens efficiently by alternating proposer and verifier roles.

# 2. Background: Speculative Decoding and Its Extension

<div class="figure" id="fig1"> 
<img src="https://kamichanw.github.io/files/cos/fig1.png" alt="Figure 1" />
<p><i>Figure 1.</i> (a) Vanilla collaborative decoding, (b) speculative decoding, (c) CoS (Naive-CoS variant).</p>
</div>

## 2.1 Limitations of Vanilla Collaborative Decoding

Standard collaborative decoding computes the token distribution using functions like weighted averaging or contrastive subtraction:

\\[
r_i(x) = \sum_{k=1}^n \lambda_k p_i^{(k)}(x)
\\]
This requires \\(n\\) forward passes per token, causing high latency as shown in <a href="#fig1">Figure 1(a)</a>.

## 2.2 Speculative Decoding Revisited  
Speculative Decoding (SD) accelerates generation using:
- A **proposal model** \\( \mathcal{M}_q \\) to generate candidate tokens.
- A **verifier model** \\( \mathcal{M}_p \\) to verify them in parallel.

Given \\( \gamma \\) proposal tokens \\( x_{i+1}, \ldots, x_{i+\gamma} \\), verification is based on:
\\[
u_j \leq \min\left(1, \frac{p_{i+j}(x)}{q_{i+j}(x)}\right)
\\]
Rejected tokens are resampled from a renormalized residual distribution.

# 3. Collaborative Decoding via Speculation (CoS)

## 3.1 Naive-CoS: Speculative Decoding with Combined Distribution  
Naive-CoS extends SD by verifying tokens using a **combined distribution** \\(r(x)\\):
- For weighted ensemble (WE):  
  \\[
  r(x) = \lambda q(x) + (1-\lambda) p(x)
  \\]
- For contrastive decoding (CD):  
  \\[
  r(x) = \text{Softmax}(l_p - \mu l_q)
  \\]
Verification becomes:
\\[
u_j \leq \min\left(1, \frac{r_{i+j}(x)}{q_{i+j}(x)}\right)
\\]
This ensures generated tokens align with \\(r(x)\\), forming the foundation of CoS.

## 3.2 Alternate Proposal Framework  
<div class="figure" id="fig2"> 
<img src="https://kamichanw.github.io/files/cos/fig2.png" alt="Figure 2" />
<p><i>Figure 2.</i> Alternate Proposal Framework alternating proposer/verifier roles to utilize bonus tokens.</p>
</div>

When all proposed tokens are accepted, the verifier emits a **bonus token**. CoS treats this bonus as the next proposal, enabling the models to **alternate** proposer/verifier roles:
1. \\( \mathcal{M}_q \\) proposes \\( \gamma_q \\) tokens → verified by \\( \mathcal{M}_p \\)
2. If accepted, \\( \mathcal{M}_p \\) emits a bonus token → verified by \\( \mathcal{M}_q \\)

This **feedback loop** improves utilization and throughput.

## 3.3 Generalization to \\(n\\)-Model Collaboration  
<div class="figure" id="fig3"> 
<img src="https://kamichanw.github.io/files/cos/fig3.png" alt="Figure 3" />
<p><i>Figure 3.</i> CoS applied to three-model setting with scoring and bonus token chaining.</p>
</div>

In the \\(n\\)-model CoS:
- Each model scores proposals from others in parallel.
- Each scoring pass also emits a bonus token.
- Verification of a token happens only after being scored by all other models.

This extension preserves the theoretical efficiency guarantees.

# 4. Experimental Results

## 4.1 Setup  
We evaluate CoS on four benchmarks: **HumanEval, GSM8K, MMLU**, and **CNNDM**, across weighted ensemble and contrastive decoding.

### Compared methods:
We compare the following decoding strategies:

- Ensemble Decoding (**WE, CD**) – standard ensemble with weighted or contrastive decoding.
- Parallel Ensemble Decoding (**WE-P, CD-P**) – parallel processing version of standard ensemble methods, where each model computes its distribution independently and in parallel.
- Speculative Decoding (**WE-SD, CD-SD**) – acceleration using the smallest model as proposer and the ensemble distribution as verifier.
- Collaborative Decoding via Speculation (**WE-CoS, CD-CoS**) – our proposed method using alternating roles and speculative ensemble sampling.

### Model pairs:
<table border="1" class="results">
  <thead>
    <tr>
      <th>Name</th>
      <th>M_q</th>
      <th>M_p</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="3"><strong>Weight Ensemble (WE)</strong></td>
      <td>Llama-Vicuna</td>
      <td>Llama-2-7B, Vicuna-7B-V1.5</td>
    </tr>
    <tr>
      <td>Qwen-3b</td>
      <td>Qwen2.5-3B-Instruct, Qwen2.5-Coder-3B-Instruct</td>
    </tr>
    <tr>
      <td>Qwen-1.5b</td>
      <td>Qwen2.5-1.5B-Instruct, Qwen2.5-Coder-1.5B-Instruct</td>
    </tr>
    <tr>
      <td rowspan="3"><strong>Contrastive Decoding (CD)</strong></td>
      <td>Llama-3</td>
      <td>Llama-3.2-1B, Llama-3.1-8B-Instruct</td>
    </tr>
    <tr>
      <td>Llama-2</td>
      <td>Llama-68M, Llama-2-7B</td>
    </tr>
    <tr>
      <td>OPT</td>
      <td>OPT-125M, OPT-13B</td>
    </tr>
  </tbody>
</table>



## 4.2 Main Results
**Weighted Ensemble (WE) Performance**  
<table border="1" class="results">
  <thead>
    <tr>
      <th>Model</th>
      <th>Method</th>
      <th>HumanEval</th>
      <th>GSM8K</th>
      <th>MMLU</th>
      <th>CNNDM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="4">Llama-Vicuna</td>
      <td>WE</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
    </tr>
    <tr>
      <td>WE-P</td>
      <td>0.69x</td>
      <td>0.73x</td>
      <td>0.70x</td>
      <td>0.75x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>1.27x</td>
      <td>1.21x</td>
      <td>1.19x</td>
      <td>1.15x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.58x</td>
      <td>1.52x</td>
      <td>1.41x</td>
      <td>1.46x</td>
    </tr>
    <tr>
      <td rowspan="4">Qwen-3b</td>
      <td>WE</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
    </tr>
    <tr>
      <td>WE-P</td>
      <td>0.74x</td>
      <td>0.79x</td>
      <td>0.79x</td>
      <td>0.77x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>1.13x</td>
      <td>1.06x</td>
      <td>1.09x</td>
      <td>1.08x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.62x</td>
      <td>1.52x</td>
      <td>1.42x</td>
      <td>1.38x</td>
    </tr>
    <tr>
      <td rowspan="4">Qwen-1.5b</td>
      <td>WE</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
    </tr>
    <tr>
      <td>WE-P</td>
      <td>0.63x</td>
      <td>0.62x</td>
      <td>0.64x</td>
      <td>0.63x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>1.11x</td>
      <td>1.13x</td>
      <td>1.08x</td>
      <td>1.10x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.56x</td>
      <td>1.46x</td>
      <td>1.34x</td>
      <td>1.35x</td>
    </tr>
    <tr>
      <td rowspan="4">Qwen-1.5b (3 Model)</td>
      <td>WE</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
      <td>1.00x</td>
    </tr>
    <tr>
      <td>WE-P</td>
      <td>0.54x</td>
      <td>0.73x</td>
      <td>0.80x</td>
      <td>0.82x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>0.96x</td>
      <td>0.92x</td>
      <td>0.98x</td>
      <td>0.95x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.85x</td>
      <td>1.53x</td>
      <td>1.38x</td>
      <td>1.27x</td>
    </tr>
  </tbody>
</table>


**Contrastive Decoding (CD) Performance**  
<table border="1" class="results">
  <thead>
    <tr>
      <th>Model</th>
      <th>T</th>
      <th>Method</th>
      <th>HumanEval</th>
      <th>GSM8K</th>
      <th>MMLU</th>
      <th>CNNDM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="8">Llama-3</td>
      <td rowspan="4">0</td>
      <td>CD</td>
      <td>1.00x</td><td>1.00x</td><td>1.00x</td><td>1.00x</td>
    </tr>
    <tr>
      <td>CD-P</td>
      <td>0.41x</td><td>0.40x</td><td>0.41x</td><td>0.41x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>2.04x</td><td>1.81x</td><td>1.52x</td><td>1.58x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>2.23x</td><td>2.00x</td><td>1.77x</td><td>1.61x</td>
    </tr>
    <tr>
      <td rowspan="4">1</td>
      <td>CD</td>
      <td>1.00x</td><td>1.00x</td><td>1.00x</td><td>1.00x</td>
    </tr>
    <tr>
      <td>CD-P</td>
      <td>0.39x</td><td>0.41x</td><td>0.42x</td><td>0.41x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>1.55x</td><td>1.21x</td><td>1.20x</td><td>1.07x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.65x</td><td>1.44x</td><td>1.31x</td><td>1.18x</td>
    </tr>
    <tr>
      <td rowspan="8">Llama-2</td>
      <td rowspan="4">0</td>
      <td>CD</td>
      <td>1.00x</td><td>1.00x</td><td>1.00x</td><td>1.00x</td>
    </tr>
    <tr>
      <td>CD-P</td>
      <td>0.59x</td><td>0.50x</td><td>0.54x</td><td>0.48x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>1.15x</td><td>1.62x</td><td>1.08x</td><td>0.93x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.26x</td><td>1.65x</td><td>1.68x</td><td>1.30x</td>
    </tr>
    <tr>
      <td rowspan="4">1</td>
      <td>CD</td>
      <td>1.00x</td><td>1.00x</td><td>1.00x</td><td>1.00x</td>
    </tr>
    <tr>
      <td>CD-P</td>
      <td>0.56x</td><td>0.51x</td><td>0.53x</td><td>0.49x</td>
    </tr>
    <tr>
      <td>SD</td>
      <td>0.94x</td><td>1.16x</td><td>1.23x</td><td>1.10x</td>
    </tr>
    <tr class="highlight">
      <td>CoS</td>
      <td>1.15x</td><td>1.20x</td><td>1.37x</td><td>1.11x</td>
    </tr>
  </tbody>
</table>






<style>
.model-config, .results {
  width: 100%;
  border-collapse: collapse;
  margin: 20px 0;
}
.model-config th, .model-config td, .results th, .results td {
  border: 1px solid #ddd;
  padding: 8px;
  text-align: center;
}
.results th {
  background: #f8f9fa;
}
.highlight {
  background-color: #e3f2fd;
  font-weight: bold;
}
.figure {
  text-align: center;
  margin: 20px 0;
}
.figure img {
  max-width: 80%;
  border: 
}
.figure p {
  text-align: left;
  font-size: small;
}
</style>
  
